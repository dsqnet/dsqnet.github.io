<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DSQNet</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://seungyeon-k.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://sqpdnet.github.io">
            [CoRL 2022] SQPDNet
          </a>
          <a class="navbar-item" href="https://searchforgrasp.github.io">
            [CoRL 2023] Search-for-Grasp
          </a>
          <a class="navbar-item" href="https://t2sqnet.github.io">
            [CoRL 2024] T<sup>2</sup>SQNet
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <!-- Title. -->
          <h1 class="title is-2 publication-title">DSQNet: A Deformable Model-Based Supervised Learning Algorithm for Grasping Unknown Occluded Objects</h1>
          <p class="subtitle">Transactions on Automation Science and Engineering (T-ASE) 2022</p>
          
          <!-- Authors. -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://seungyeon-k.github.io">Seungyeon Kim</a><sup>*, 1</sup>,</span>
            <span class="author-block">
              Taegyun Ahn<sup>*, 2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.gabe-yhlee.com">Yonghyeon Lee</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Jihwan Kim<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Oo7c22wAAAAJ&hl=ko">Michael Yu Wang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/robotics.snu.ac.kr/fcp/">Frank C. Park</a><sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University,</span>
            <span class="author-block"><sup>2</sup>Saige Research,</span>
            <span class="author-block"><sup>3</sup>Monash University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
          </div>
          
          <!-- Icons. -->
          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://ieeexplore.ieee.org/abstract/document/9802912?casa_token=DDg8CXt8Z2gAAAAA:sYnwNn9vX2Vl7Y1p68zkqSuPKvYbKX8q8lJgae29uXXt05PcVjU78GI127Knz1X888rv85l-4Q"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=OOgsh6-h62g"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/seungyeon-k/DSQNet-public"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>
          </div>
          
          <!-- Blank. -->
          <div class="columns is-centered">
            <div class="content">
              <h2 class="title is-3"></h2>
            </div>            
          </div>

          <!-- TL;DR. -->
          <div class="columns is-centered has-text-justified interpolation-panel">
            <div class="column is-full-width">
              <h2 class="title is-4">
                <font color="#808080">
                  <p></p><p></p><p></p>
                  TL;DR: This paper presents a recognition model for grasping that 
                  combines deformable superquadrics with a deep learning network, DSQNet, to infer complete 
                  object shapes from partially observed point clouds.
                </font>
              </h2>
            </div>
          </div>
          
          <!-- Blank. -->
          <div class="columns is-centered">
            <div class="content">
              <h2 class="title is-3"></h2>
            </div>            
          </div>

          <!-- Teaser Video. -->
          <div class="content has-text-left">
            <h2 class="title is-3">Robotic Grasping</h2>
            <p>
              Our recognition model-based method can accurately recognize objects as one or more deformable 
              superquadrics and effectively find grasp poses from the recognized shapes.
            </p>
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <div class="content">
                  <p>
                    <b>Grasping objects with single-part</b>
                  </p>
                  <video id="sd" autoplay controls muted playsinline height="100%">
                    <source src="./assets/videos/single.mp4"
                            type="video/mp4">
                  </video>
                </div>
              </div>

              <div class="column has-text-centered">
                <div class="content">
                  <p>
                    <b>Grasping objects with multi-parts</b>
                  </p>
                  <video id="sd" autoplay controls muted playsinline height="100%">
                    <source src="./assets/videos/multi.mp4"
                            type="video/mp4">
                  </video>
                </div>
              </div>
              
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Grasping previously unseen objects for the first time, in which only
            partially occluded views of the object are available, remains a difficult
            challenge.  Despite their recent successes, deep learning-based
            end-to-end methods remain impractical when training data and resources
            are limited and multiple grippers are used.  Two-step methods that first
            identify the object shape and structure using deformable shape templates,
            then plan and execute the grasp, are free from those limitations, but also have
            difficulty with partially occluded objects.  In this paper, we propose a
            two-step method that merges a richer set of shape primitives, the
            deformable superquadrics, with a deep learning network, DSQNet,
            that is trained to identify complete object shapes from partial point
            cloud data. Grasps are then generated that take into account the
            kinematic and structural properties of the gripper while exploiting the
            closed-form equations available for deformable superquadrics. A
            seven-dof robotic arm equipped with a parallel jaw gripper is used to
            conduct experiments involving a collection of household objects,
            achieving average grasp success rates of 93% (compared to 86% for
            existing methods), with object recognition times that are ten times
            faster.
          </p>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/OOgsh6-h62g?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Recognition Model. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview</h2>

        <div class="content has-text-justified">
          <p>
            Our recognition-based grasping method proceeds in three steps as shown in
            the below figure: (i) a trained segmentation
            network is used to segment a partially observed point cloud into a set of simpler point
            clouds; (ii) The trained DSQNet converts each point cloud into a deformable
            superquadric primitive, with its collective union representing the full object
            shape; (iii) grasp poses are generated in a gripper-dependent manner from the
            recognized full shapes.
            
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/overview.PNG"
                 alt="overview"/>
          </div>
        </div>
   
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Recognition Model. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Recognition Model</h2>

        <!-- SE(2)-Equivariant Pushing Dynamics Models. -->
        <h3 class="title is-4">Deformable Superquadrics</h3>
        <div class="content has-text-justified">
          <p>

            <b>Superquadrics</b> are an extended set of quadric surfaces that can be
            used to represent diverse shapes ranging from boxes, cylinders, and
            ellipsoids to bi-cones, octahedra, and other complex symmetric shapes,
            even those with rounded corners and edges. Superquadrics are further
            categorized into superellipsoids, superhyperboloids, and supertoroids;
            for our modeling purposes the <b>superellipsoids</b> are sufficient. The
            corresponding implicit equations for a superquadric surface with size parameters 
            \((a_1, a_2, a_3) \in \mathbb{R}_+^3\) and shape parameters \((e_1, e_2) \in \mathbb{R}_+^2\)
            are of the form: for \(\textbf{x} = (x, y, z)\), 
            $$
            \begin{equation*}
              f(\textbf{x})=\left(\left|\frac{x}{a_1}\right|^{\frac{2}{e_2}}
              + \left|\frac{y}{a_2}\right|^{\frac{2}{e_2}}\right)^{\frac{e_2}{e_1}}
              + \left|\frac{z}{a_3}\right|^{\frac{2}{e_1}} = 1.
              \end{equation*}
            $$
            Although more expressive than quadrics, superquadrics are still limited
            by their inability to capture tapered and bent objects. <b>Deformable
            superquadrics</b> are obtained by applying global tapering and bending
            deformations as shown below.
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/dsq.PNG"
                 alt="dsq"/>
          </div>
        </div>

        <!-- SE(2)-Equivariant Pushing Dynamics Models. -->
        <h3 class="title is-4">DSQNet: Deformable Superquadric Network</h3>
        <div class="content has-text-justified">
          <p>
            We design a neural network architecture, referred to as the <b>Deformable Superquadric Network 
            (DSQNet)</b>, which takes partially observed point cloud data as input and outputs the eight 
            parameters and the pose of the deformable superquadric. This output aims to reconstruct the 
            full shape of the object, including its occluded parts. The network is trained to minimize 
            fitting errors between the ground-truth point cloud and the predicted deformable superquadric.
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/dsqnet.PNG"
                 alt="dsqnet"/>
          </div>
        </div>

        <!-- Pushing Dynamics Learning. -->
        <h3 class="title is-4">Recognition Results</h3>
        <div class="content has-text-justified">
          <p>
            Recognition is achieved quickly and accurately with a simple forward pass through the neural network. 
            Extensive experiments and benchmark comparisons using a variety of everyday objects demonstrate both 
            the strengths of our approach and potential areas for improvement. For recognizing household objects, 
            our method achieves the highest accuracy (in terms of volumetric IoU) and the fastest computation speeds 
            among existing recognition-based methods.
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./assets/images/recognition.PNG"
                 alt="recognition results"/>
          </div>
        </div>
   
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Pushing Manipulation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Object Grasping</h2>

        <!-- Object Manipulation with T2SQNet. -->
        <h3 class="title is-4">Object Grasping with DSQNet</h3>
        <div class="content has-text-justified">
          <p>
            Once the shape of an object is recognized as a set of deformable superquadric primitives, conventional grasp 
            pose generation techniques can be applied for a given gripper. In this paper, we focus on parallel jaw grippers
            and adopt an antipodal points sampling-based grasp pose generation method. We first develop an efficient algorithm 
            for sampling antipodal points on deformable superquadric primitives, followed by a grasp pose generation algorithm 
            that utilizes the sampled pairs of antipodal points.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="tablewarenet" autoplay controls muted loop playsinline height="100%">
            <source src="./assets/videos/grasping.mp4"
                    type="video/mp4">
          </video>
        </div>


        <!-- Object Manipulation Results. -->
        <h3 class="title is-4"> Shape Uncertainty-Aware Object Grasping</h3>
        <div class="content has-text-justified">
          <p>
            We have additionally developed a shape uncertainty-aware grasping algorithm to assist in grasp pose planning and enhance 
            grasp performance in more challenging scenarios. While DSQNet achieves the best recognition performance on both 
            synthetic and real-world objects, perfect shape recognition is not always feasible. If the generated grasp pose 
            targets an erroneous part of the recognized shape, the gripper may fail to grasp the object. To mitigate the risk 
            of grasping incorrect parts, we introduce a new grasp score that accounts for shape uncertainty.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="tablewarenet" autoplay controls muted loop playsinline height="100%">
            <source src="./assets/videos/uncertainty.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Citation. -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Citation</h2>
    <pre><code>
@article{kim2022dsqnet,
  title={DSQNet: a deformable model-based supervised learning algorithm for grasping unknown occluded objects},
  author={Kim, Seungyeon and Ahn, Taegyun and Lee, Yonghyeon and Kim, Jihwan and Wang, Michael Yu and Park, Frank C},
  journal={IEEE Transactions on Automation Science and Engineering},
  volume={20},
  number={3},
  pages={1721--1734},
  year={2022},
  publisher={IEEE}
}
    </code></pre>
  </div>
</section>

<!-- Footer. -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template modified from <a href="https://nerfies.github.io/">NeRFies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
